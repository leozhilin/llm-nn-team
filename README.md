# Lowering Costs and Increasing Benefits through the Ensemble of LLMs and Machine Learning Models

## Abstract

Due to the remarkable capabilities of generative AI technologies, pre-trained large language models (LLMs) have been widely used. However, the high computational and memory requirements limit the widespread adoption of LLMs. It is a challenging issue that how to reduce the cost of using LLMs while maintaining their performance as much as possible. In this paper, we explore to integrate LLMs with small-scale machine learning models through ensemble learning. We propose a collaborative training and testing framework, which mainly includes a LLM, a small machine learning model and a task allocator. In this framework, we can train the small machine learning model, and further train the task allocator using the data generated by both the LLM and the small machine learning model. The task allocator is trained to learn how to assign tasks to the LLM or the small machine learning model. We design the training method of task allocator and give its implementation algorithm. To validate our method, we conducted experiments on two tasks, sentiment classification and text summarization. The results demonstrate that our approach can improve the performance while reducing the overall usage costs.

## Keywords

Large Language Models, Small-scale Machine Learning Models, Ensemble Learning.

## Experiments

### OLHS Task

#### step 1:Preparation

download and unzip glove.6B.100d.txt from Stanford University

```shell
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
```

download labelled_data.npy from Github Repo of Towards Unbiased and Accurate Deferral to Multiple Experts by Keswani et al. (2021)

```
#download from https://github.com/vijaykeswani/Deferral-To-Multiple-Experts/blob/main/output/labelled_data.npy
```

```
pip install pandas
pip install sklearn
```



#### step 2:Data preprocessing and integration of results generated by LLM

You need to process the dataset in the following format and use any LLM to generate predictions, then integrate them into a JSON file.

Offline Large Language Model：

```python
[
    {
        "Id": "Sample ID, this is not mandatory.",
        "Text": "content of data",
        "Label": "label provided in dataset",
        "Preds": "prediction generated by LLM",
        "Time": "time cost of LLM's prediction"
    }
]
```

We provide our codes, which are`ChatGLM3_generate_llm_preds.py`, `Qwen_generate_llm_preds.py`, `InternLM2_generate_llm_preds.py`. If you want to run them in practice, you'll need to set up and deploy [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3), [Qwen1.5-7B](https://github.com/QwenLM/Qwen1.5), [InternLM2-7B](https://github.com/InternLM/InternLM) locally as required, and make modifications to the code's large model invocation section.

Onine Large Language Model：

```python
[
    {
        "Id": "Sample ID, this is not mandatory.",
        "Text": "content of data",
        "Label": "label provided in dataset",
        "Preds": "prediction generated by LLM",
        "Completion_tokens": "Completion tokens",
        "Prompt_tokens": "Prompt tokens"
    }
]
```

#### step 3：Execution

To run LLM_NN_Team.py, you need to specify the following two paths in the code beforehand.

```python
#You need to replace this path with the actual path where the GloVe file is located.
PATH_GLOVE_MODEL = '../data/glove.6B.100d.txt' 

#You need to replace this path with the path where the data file is located.
OLHS_WITH_PREDS_PATH = 'OLHS_data_ChatGLM3.json' 
```

### Summarization Task

#### step 1:Preparation

download XSum dataset

```
#download from https://drive.google.com/file/d/1SI7dafnXvcp96nQzhgMgafOso0XUmGFo/view
```

#### step 2:Data preprocessing and integration of results generated by LLM

You need to process the dataset in the following format, utilize any Large Language Model to generate predictions, and then integrate them into a JSON file.

Offline Large Language Model：

```python
#Note that unlike the handling of OLHS, each piece of data is a dictionary, occupying one line when stored in the file, and not encapsulated as an element within a list.
{
        "id": "Sample ID, this is not mandatory.",
        "input": "content of data",
        "ground_truth": "reference provided in dataset",
        "generation": "Summary generated by LLM",
        "time": "time spent by LLM"
}
```

Our codes are `generate_llm_summarization.py`. If you want to run them in practice, you'll need to set up and deploy [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3), [Qwen1.5-7B](https://github.com/QwenLM/Qwen1.5), [InternLM2-7B](https://github.com/InternLM/InternLM) locally as required, and make modifications to the code's large model invocation section.

#### step 3:Integrating results generated by pre-trained small-scale ML models

Next, based on the file generated in step 2, additionally integrate the results generated by pre-trained small ML models in the following format.

```python
#Note that unlike the handling of OLHS, each piece of data is a dictionary, occupying one line when stored in the file, and not encapsulated as an element within a list.
{
        "id": "Sample ID, this is not mandatory.",
        "input": "content of data",
        "ground_truth": "reference provided in dataset",
        "generation": "Summary generated by LLM",
        "time": "time spent by LLM",
        "nn_generation": "Summary generated by pre-trained neural network",
        "nn_time": "time spent by pre-trained neural network"
}
```

#### step 4:Execution

To run LLM_NN_Team.py, you need to specify the following two paths in the code beforehand.

```python
#The data file processed according to the above steps.
DATA_PATH = './NN.json'

#The path to the GloVe file.
PATH_GLOVE_MODEL = '../data/glove.6B.100d.txt'
```

