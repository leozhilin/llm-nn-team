# Enhanced Cost Efficiency: Leveraging the Ensemble of LLM with Small-scale Machine Learning Models

## Abstract

Due to the remarkable capabilities of current generative AI technologies, pre-trained large language models (LLMs) have been widely employed in various applications. However, the high computational and memory requirements limit the widespread adoption of LLMs. Consequently, an increasing number of researchers have begun investigating ways to reduce the cost of using LLMs while maintaining their performance as much as possible. In this paper, we examine the feasibility of integrating LLMs with small-scale machine learning models through ensemble learning. We propose a collaborative training framework and allocation algorithm, and conduct experiments on sentiment inference and text summarization datasets, which serve as representatives of common text classification and generation tasks for large models, respectively. The results demonstrate that our approach can enhance the performance of large models while reducing overall usage costs by decreasing their invocation frequency.

## Experiments

### OLHS Task

#### step 1:Preparation

download and unzip glove.6B.100d.txt from Stanford University

```shell
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
```

download labelled_data.npy from Github Repo of Towards Unbiased and Accurate Deferral to Multiple Experts by Keswani et al. (2021)

```
#download from https://github.com/vijaykeswani/Deferral-To-Multiple Experts/blob/3aa1f621991cb9d7823757d68c178870b686da62/output/labelled_data.npy
```

#### step 2:Data preprocessing and integration of results generated by LLM

You need to process the dataset in the following format and use any LLM to generate predictions, then integrate them into a JSON file.

Offline Large Language Model：

```python
[
    {
        "Id": "Sample ID, this is not mandatory.",
        "Text": "content of data",
        "Label": "label provided in dataset",
        "Preds": "prediction generated by LLM",
        "Time": "time cost of LLM's prediction"
    }
]
```

We provide our codes, which are`ChatGLM3_generate_llm_preds.py`, `Qwen_generate_llm_preds.py`, `InternLM2_generate_llm_preds.py`. If you want to run them in practice, you'll need to set up and deploy [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3), [Qwen1.5-7B](https://github.com/QwenLM/Qwen1.5), [InternLM2-7B](https://github.com/InternLM/InternLM) locally as required, and make modifications to the code's large model invocation section.

Onine Large Language Model：

```python
[
    {
        "Id": "Sample ID, this is not mandatory.",
        "Text": "content of data",
        "Label": "label provided in dataset",
        "Preds": "prediction generated by LLM",
        "Completion_tokens": "Completion tokens",
        "Prompt_tokens": "Prompt tokens"
    }
]
```

#### step 3：Execution

To run LLM_NN_Team.py, you need to specify the following two paths in the code beforehand.

```python
#You need to replace this path with the actual path where the GloVe file is located.
PATH_GLOVE_MODEL = '../data/glove.6B.100d.txt' 

#You need to replace this path with the path where the data file is located.
OLHS_WITH_PREDS_PATH = 'OLHS_data_ChatGLM3.json' 
```

### Summarization Task

#### step 1:Preparation

download XSum dataset

```
#download from https://drive.google.com/file/d/1SI7dafnXvcp96nQzhgMgafOso0XUmGFo/view
```

#### step 2:Data preprocessing and integration of results generated by LLM

You need to process the dataset in the following format, utilize any Large Language Model to generate predictions, and then integrate them into a JSON file.

Offline Large Language Model：

```python
#Note that unlike the handling of OLHS, each piece of data is a dictionary, occupying one line when stored in the file, and not encapsulated as an element within a list.
{
        "id": "Sample ID, this is not mandatory.",
        "input": "content of data",
        "ground_truth": "reference provided in dataset",
        "generation": "Summary generated by LLM",
        "time": "time spent by LLM"
}
```

Our codes are `generate_llm_summarization.py`. If you want to run them in practice, you'll need to set up and deploy [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3), [Qwen1.5-7B](https://github.com/QwenLM/Qwen1.5), [InternLM2-7B](https://github.com/InternLM/InternLM) locally as required, and make modifications to the code's large model invocation section.

#### step 3:Integrating results generated by pre-trained small-scale ML models

Next, based on the file generated in step 2, additionally integrate the results generated by pre-trained small ML models in the following format.

```python
#Note that unlike the handling of OLHS, each piece of data is a dictionary, occupying one line when stored in the file, and not encapsulated as an element within a list.
{
        "id": "Sample ID, this is not mandatory.",
        "input": "content of data",
        "ground_truth": "reference provided in dataset",
        "generation": "Summary generated by LLM",
        "time": "time spent by LLM",
        "nn_generation": "Summary generated by pre-trained neural network",
        "nn_time": "time spent by pre-trained neural network"
}
```

#### step 4:Execution

To run LLM_NN_Team.py, you need to specify the following two paths in the code beforehand.

```python
#The data file processed according to the above steps.
DATA_PATH = './NN.json'

#The path to the GloVe file.
PATH_GLOVE_MODEL = '../data/glove.6B.100d.txt'#GloVe文件所在的路径
```

